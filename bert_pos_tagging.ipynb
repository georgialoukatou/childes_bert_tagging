{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgialoukatou/childes_bert_tagging/blob/main/bert_pos_tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9HA3e3ihLUt",
        "outputId": "73e7d808-66ec-4b0c-ec73-d36e07ee9aff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.49 s (started: 2022-01-21 00:13:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#https://pageperso.lis-lab.fr/benoit.favre/pstaln/09_embedding_evaluation.html\n",
        "#https://colab.research.google.com/drive/1sbfjIapc1MDcQpGYlLkapjhWWv3yOR9t#scrollTo=hW7aY1VWtbzE\n",
        "#https://colab.research.google.com/drive/1PHv-IRLPCtv7oTcIGbsgZHqrB5LPvB7S#scrollTo=IdU4YVqb7N8M\n",
        "\n",
        "!pip install -qq transformers\n",
        "!pip -q install conllu\n",
        "\n",
        "import conllu\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import time\n",
        "import torch\n",
        "import re\n",
        "import collections\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import sys\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
        "#torch.cuda.is_available()\n",
        "#torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj5NjxzvCBDA",
        "outputId": "6f2f00cd-6f9e-4d51-b6ce-06893b805b95"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.16 ms (started: 2022-01-21 00:13:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "HZndHzoFV7Da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4feaa3dd-58f8-440c-dd9c-441dcedf4021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n",
            "The autotime extension is already loaded. To reload it, use:\n",
            "  %reload_ext autotime\n",
            "time: 2.7 s (started: 2022-01-21 00:13:16 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import english hand annotated CHILDES sample\n",
        "eng_gold_childes = pd.read_csv('https://raw.githubusercontent.com/georgialoukatou/childes_bert_tagging/main/eng_gold_childes.csv')\n",
        "eng_gold_childes = eng_gold_childes.dropna(axis=0, subset=['correct_pos']) \n",
        "mapping = {'mod': 'AUX', 'prep': 'ADP','adj': 'ADJ', 'pro': 'PRON', 'v': 'VERB', 'adv': 'ADV', 'participle': 'VERB', 'n': 'NOUN', 'childes': 'X', 'nan': 'X', 'intj': 'INTJ', 'particle': 'PART', 'mod': 'AUX','mod ': 'AUX', 'spacy': 'X', 'on': 'X', 'participle': 'VERB', 'aux': 'AUX', 'num': 'NUM', 'det':'DET' }\n",
        "#for English, missing PROPN, CCONJ, SCONJ, SYM, PUNCT\n",
        "eng_gold_childes = eng_gold_childes.replace({'correct_pos': mapping})\n",
        "eng_gold_childes_test = eng_gold_childes[['utterance_gloss','correct_pos', 'position']]\n",
        "print(eng_gold_childes_test.head())\n",
        "print(eng_gold_childes_test.correct_pos.unique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaL4Wau_-p4W",
        "outputId": "5796e08f-d3c0-4a3f-969e-694eb1721efc"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     utterance_gloss correct_pos  position\n",
            "0  the shortcut led right to where Harold thought...         AUX      11.0\n",
            "1  I'm not allowing you to reach things onto the ...         ADP       9.0\n",
            "2                                      I dare not go         AUX       1.0\n",
            "3                        yyy knee behind the letters         ADP       4.0\n",
            "4                    is that kind of a greenish blue         ADJ       5.0\n",
            "['AUX' 'ADP' 'ADJ' 'PRON' 'VERB' 'ADV' 'NOUN' 'X' 'INTJ' 'PART' 'NUM'\n",
            " 'DET']\n",
            "time: 2.11 s (started: 2022-01-21 00:58:29 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in ['en_ewt-ud-train.conllu', 'en_ewt-ud-dev.conllu', 'en_ewt-ud-test.conllu']:\n",
        "  urllib.request.urlretrieve('https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/' + filename, filename)\n",
        "\n",
        "def load_conllu(filename):\n",
        "  with open(filename) as fp:\n",
        "    data = conllu.parse(fp.read())\n",
        "  sentences = [[token['form'] for token in sentence] for sentence in data]\n",
        "  taggings = [[token['upos'] for token in sentence] for sentence in data]\n",
        "  return sentences, taggings\n",
        "\n",
        "train_sentences, train_taggings = load_conllu('en_ewt-ud-train.conllu')\n",
        "valid_sentences, valid_taggings = load_conllu('en_ewt-ud-dev.conllu')\n",
        "test_sentences, test_taggings = load_conllu('en_ewt-ud-test.conllu')\n",
        "\n",
        "ud_train=pd.DataFrame(columns=list('AB'))\n",
        "for i in zip(train_sentences, train_taggings):\n",
        "  ud_train = ud_train.append({'A': i[0],'B': i[1]}, ignore_index=True)\n",
        "\n",
        "ud_valid=pd.DataFrame(columns=list('AB'))\n",
        "for i in zip(valid_sentences, valid_taggings):\n",
        "  ud_valid = ud_valid.append({'A': i[0],'B': i[1]}, ignore_index=True)\n",
        "\n",
        "ud_test=pd.DataFrame(columns=list('AB'))\n",
        "for i in zip(test_sentences, test_taggings):\n",
        "  ud_test = ud_test.append({'A': i[0],'B': i[1]}, ignore_index=True)\n",
        "\n",
        "ud_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "HLTSNEGedhC0",
        "outputId": "9e0e3336-1dd6-4e47-d21c-28e7e69eb1c1"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-21e42981-cb22-429b-a54f-6bb649e67bdc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Al, -, Zaman, :, American, forces, killed, Sh...</td>\n",
              "      <td>[PROPN, PUNCT, PROPN, PUNCT, ADJ, NOUN, VERB, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[, This, killing, of, a, respected, cleric, w...</td>\n",
              "      <td>[PUNCT, DET, NOUN, ADP, DET, ADJ, NOUN, AUX, A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[DPA, :, Iraqi, authorities, announced, that, ...</td>\n",
              "      <td>[PROPN, PUNCT, ADJ, NOUN, VERB, SCONJ, PRON, A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Two, of, them, were, being, run, by, 2, offic...</td>\n",
              "      <td>[NUM, ADP, PRON, AUX, AUX, VERB, ADP, NUM, NOU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[The, MoI, in, Iraq, is, equivalent, to, the, ...</td>\n",
              "      <td>[DET, PROPN, ADP, PROPN, AUX, ADJ, ADP, DET, P...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21e42981-cb22-429b-a54f-6bb649e67bdc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-21e42981-cb22-429b-a54f-6bb649e67bdc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-21e42981-cb22-429b-a54f-6bb649e67bdc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   A                                                  B\n",
              "0  [Al, -, Zaman, :, American, forces, killed, Sh...  [PROPN, PUNCT, PROPN, PUNCT, ADJ, NOUN, VERB, ...\n",
              "1  [[, This, killing, of, a, respected, cleric, w...  [PUNCT, DET, NOUN, ADP, DET, ADJ, NOUN, AUX, A...\n",
              "2  [DPA, :, Iraqi, authorities, announced, that, ...  [PROPN, PUNCT, ADJ, NOUN, VERB, SCONJ, PRON, A...\n",
              "3  [Two, of, them, were, being, run, by, 2, offic...  [NUM, ADP, PRON, AUX, AUX, VERB, ADP, NUM, NOU...\n",
              "4  [The, MoI, in, Iraq, is, equivalent, to, the, ...  [DET, PROPN, ADP, PROPN, AUX, ADJ, ADP, DET, P..."
            ]
          },
          "metadata": {},
          "execution_count": 156
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 40.7 s (started: 2022-01-21 00:13:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_gold(items):\n",
        "  #max_len = max(len(item[0]) for item in items)\n",
        "  max_len=512\n",
        "  sentences = torch.zeros((len(items), max_len), device=items[0][0].device).long().to(device)\n",
        "  #taggings = torch.zeros((len(items), max_len)).long().to(device)\n",
        "\n",
        "  for i, sentence in enumerate(items):\n",
        "    sentences[i][0:len(sentence)] = sentence\n",
        "    #taggings[i][0:len(tagging)] = tagging\n",
        "\n",
        "  return sentences#, taggings\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvXuvrn-HvOj",
        "outputId": "fcc18f7b-6377-4f00-fc45-4124a5074eb7"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.87 ms (started: 2022-01-21 00:14:02 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A3fzdaIpdsW",
        "outputId": "63f08168-28bf-4e2f-98a8-2d601afdf479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 158
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.72 s (started: 2022-01-21 00:14:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# load tokenizer for a specific bert model (bert-base-cased)\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # tokenizer from English model better for english than multilingual\n",
        "# tokenize an example sentence\n",
        "tokenizer.tokenize('This tokenizer is sooooo awesome.')\n",
        "\n",
        "# load a specific bert model (bert-base-cased)\n",
        "bert=BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# tensor length\n",
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "max_input_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckVRPgef3Kg7",
        "outputId": "a01e02d0-81d1-4e5f-d979-1d3e25327869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'DET', 'ADP', 'AUX', 'PRON', 'PART', 'SCONJ', 'NUM', 'ADV', 'CCONJ', '_', 'X', 'INTJ', 'SYM']\n",
            "time: 67.6 ms (started: 2022-01-21 00:14:04 +00:00)\n"
          ]
        }
      ],
      "source": [
        "uniq_taggings=[]\n",
        "for i in ud_train['B']:\n",
        "  for y in i:\n",
        "    if y not in uniq_taggings:\n",
        "      uniq_taggings.append(y)\n",
        "\n",
        "print(uniq_taggings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "sWXnLKfXd6fm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "6949d44a-61f4-4191-a557-f3bd0c91ada8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c1758e4d-7638-4f0b-a37e-87bb7261fba4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10010</th>\n",
              "      <td>[I, also, understand, that, weekend, staffs, a...</td>\n",
              "      <td>[PRON, ADV, VERB, SCONJ, NOUN, NOUN, AUX, ADJ,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1758e4d-7638-4f0b-a37e-87bb7261fba4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c1758e4d-7638-4f0b-a37e-87bb7261fba4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c1758e4d-7638-4f0b-a37e-87bb7261fba4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                       A                                                  B\n",
              "10010  [I, also, understand, that, weekend, staffs, a...  [PRON, ADV, VERB, SCONJ, NOUN, NOUN, AUX, ADJ,..."
            ]
          },
          "metadata": {},
          "execution_count": 160
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 17.6 ms (started: 2022-01-21 00:14:04 +00:00)\n"
          ]
        }
      ],
      "source": [
        "ud_train['A'][41]\n",
        "ud_train.loc[[10010]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyDEhV5ZeI7b",
        "outputId": "a8fd91d5-6923-4c5d-9506-ce3e48b59b80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 741 µs (started: 2022-01-21 00:14:04 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#for sentence,tagging in zip(df_long['A'],df_long['B']):\n",
        " # tokenized_s = tokenizer.tokenize(' '.join(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "felTiyKdk8HJ",
        "outputId": "3ba8342b-4748-418f-d5d6-93837a834f4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 21.8 ms (started: 2022-01-21 00:14:04 +00:00)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def align_tokenizations(sentences, taggings):\n",
        "  bert_tokenized_sentences = []\n",
        "  aligned_taggings = []\n",
        "\n",
        "  for sentence, tagging in zip(sentences, taggings):\n",
        "    sentence = (map(lambda x: x.lower(), sentence))\n",
        "    sentence = list(sentence)\n",
        "    # first generate BERT-tokenization\n",
        "    bert_tokenized_sentence = tokenizer.tokenize(' '.join(sentence))\n",
        "    aligned_tagging = []\n",
        "    current_word = ''\n",
        "    index = 0 # index of current word in sentence and tagging\n",
        "    for token in bert_tokenized_sentence:\n",
        "      current_word += re.sub(r'^##', '', token) # recompose word with subtoken\n",
        "      sentence[index] = sentence[index].replace('\\xad', '') # fix bug in data\n",
        "      # note that some word factors correspond to unknown words in BERT\n",
        "      assert token == '[UNK]' or sentence[index].startswith(current_word)\n",
        "      if token == '[UNK]' or sentence[index] == current_word: # if we completed a word\n",
        "        current_word = ''\n",
        "        aligned_tagging.append(tagging[index])\n",
        "        index += 1\n",
        "      else: # otherwise insert padding\n",
        "        aligned_tagging.append('<pad>')\n",
        "\n",
        "    assert len(bert_tokenized_sentence) == len(aligned_tagging)\n",
        "\n",
        "    bert_tokenized_sentences.append(bert_tokenized_sentence)\n",
        "    aligned_taggings.append(aligned_tagging)\n",
        "\n",
        "  return bert_tokenized_sentences, aligned_taggings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Sj0SN3blKtP",
        "outputId": "c3ed4c0d-8927-412e-ab87-fac8675b2ecf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12536"
            ]
          },
          "metadata": {},
          "execution_count": 163
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.61 s (started: 2022-01-21 00:14:04 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#remove sentences with problematic formatting\n",
        "ud_train= ud_train.drop([157, 530, 7132])\n",
        "ud_train.drop(ud_train.index[8207:8210], inplace=True)\n",
        "ud_train.drop(ud_train.index[9974], inplace=True)\n",
        "\n",
        "#align tokenized sentences and tagging for train\n",
        "bert_tokenized_sentences, aligned_taggings = align_tokenizations(ud_train['A'], ud_train['B'])\n",
        "len(ud_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenized_sentences_test, aligned_taggings_test = align_tokenizations(ud_test['A'], ud_test['B'])\n",
        "\n",
        "ud_valid = ud_valid.drop([97, 1622])\n",
        "bert_tokenized_sentences_valid, aligned_taggings_valid = align_tokenizations(ud_valid['A'], ud_valid['B'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmIlnIBYlnWA",
        "outputId": "837caa29-f04e-4838-fca7-ce6434cd563d"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.53 s (started: 2022-01-21 00:14:09 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "ps0r3MHruikQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a266756f-c5ca-4034-dc83-27e5c692b0ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 639 ms (started: 2022-01-21 00:14:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "label_vocab = collections.defaultdict(lambda: len(label_vocab))\n",
        "label_vocab['<pad>'] = 0\n",
        "\n",
        "def convert_to_ids(sentences, taggings):\n",
        "  sentences_ids = []\n",
        "  taggings_ids = []\n",
        "  for sentence, tagging in zip(sentences, taggings):\n",
        "    sentence_tensor = torch.tensor(tokenizer.convert_tokens_to_ids(['[CLS]'] + sentence + ['[SEP]'])).long()\n",
        "    tagging_tensor = torch.tensor([0] + [label_vocab[tag] for tag in tagging] + [0]).long()\n",
        "    #sentences_ids.append(sentence_tensor.to(device))\n",
        "    sentences_ids.append(sentence_tensor)\n",
        "    #taggings_ids.append(tagging_tensor.to(device))\n",
        "    taggings_ids.append(tagging_tensor)\n",
        "  return sentences_ids, taggings_ids\n",
        "\n",
        "#convert train sentences to ids and tensors\n",
        "train_sentences_ids, train_taggings_ids = convert_to_ids(bert_tokenized_sentences, aligned_taggings)\n",
        "#same for test, dev\n",
        "test_sentences_ids, test_taggings_ids = convert_to_ids(bert_tokenized_sentences_test, aligned_taggings_test)\n",
        "valid_sentences_ids, valid_taggings_ids = convert_to_ids(bert_tokenized_sentences_valid, aligned_taggings_valid)\n",
        "\n",
        "output_dim=len(label_vocab) #dimensions of labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "0bunYtaJeHrf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6796bd4-9960-413e-8771-b7d660f5082d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18 SYM\n",
            "17 INTJ\n",
            "16 X\n",
            "15 _\n",
            "14 CCONJ\n",
            "13 ADV\n",
            "12 NUM\n",
            "11 SCONJ\n",
            "10 PART\n",
            "9 PRON\n",
            "8 AUX\n",
            "7 ADP\n",
            "6 DET\n",
            "5 VERB\n",
            "4 NOUN\n",
            "3 ADJ\n",
            "2 PUNCT\n",
            "1 PROPN\n",
            "0 <pad>\n",
            "time: 2.31 ms (started: 2022-01-21 00:14:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "for tag, count in sorted(label_vocab.items(), reverse=True, key=lambda x: x[1]):\n",
        "  print(count, tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "tbzZFZ3Cv0D_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0348731e-116e-4b65-c2d7-f9cd9887d492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512]) torch.Size([2, 512])\n",
            "time: 13.6 ms (started: 2022-01-21 00:14:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#function to be fed to the collate_fn parameter of dataloader showing how to glue data\n",
        "\n",
        "def collate_fn(items):\n",
        "  #max_len = max(len(item[0]) for item in items)\n",
        "  max_len=max_input_length\n",
        "  sentences = torch.zeros((len(items), max_len), device=items[0][0].device).long().to(device)\n",
        "  taggings = torch.zeros((len(items), max_len)).long().to(device)\n",
        "\n",
        "  for i, (sentence, tagging) in enumerate(items):\n",
        "    sentences[i][0:len(sentence)] = sentence\n",
        "    taggings[i][0:len(tagging)] = tagging\n",
        "\n",
        "  return sentences, taggings\n",
        "\n",
        "#example\n",
        "x, y = collate_fn([[torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6])], [torch.tensor([1, 2]), torch.tensor([3, 4])]])\n",
        "print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "dYtIcQVGxdgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f86637e3-1ae4-4e85-ae29-d84baf24f3d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.82 ms (started: 2022-01-21 00:14:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class PosTaggingDataset(Dataset):\n",
        "  def __init__(self, sentences, taggings):\n",
        "    assert len(sentences) == len(taggings)\n",
        "    self.sentences = sentences\n",
        "    self.taggings = taggings\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    return self.sentences[i], self.taggings[i]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "1B41uzgIkq8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8415f98a-4af0-46d5-9230-fe359470ec83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([tensor([  101,  2632,  1011, 23564,  2386,  1024,  2137,  2749,  2730, 21146,\n",
              "          28209, 14093,  2632,  1011,  2019,  2072,  1010,  1996, 14512,  2012,\n",
              "           1996,  8806,  1999,  1996,  2237,  1997,  1053,  4886,  2213,  1010,\n",
              "           2379,  1996,  9042,  3675,  1012,   102]),\n",
              "  tensor([  101,  1031,  2023,  4288,  1997,  1037,  9768, 29307,  2097,  2022,\n",
              "           4786,  2149,  4390,  2005,  2086,  2000,  2272,  1012,  1033,   102])],\n",
              " [tensor([0, 1, 2, 0, 1, 2, 3, 4, 5, 0, 1, 1, 1, 2, 0, 1, 2, 6, 4, 7, 6, 4, 7, 6,\n",
              "          4, 7, 0, 0, 1, 2, 7, 6, 3, 4, 2, 0]),\n",
              "  tensor([ 0,  2,  6,  4,  7,  6,  3,  4,  8,  8,  5,  9,  4,  7,  4, 10,  5,  2,\n",
              "           2,  0])])"
            ]
          },
          "metadata": {},
          "execution_count": 169
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 9.82 ms (started: 2022-01-21 00:14:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "example=PosTaggingDataset(train_sentences_ids, train_taggings_ids)[0:2]\n",
        "example"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.model_selection import train_test_split\n",
        "#RANDOM_SEED=1660\n",
        "\n",
        "df_train = PosTaggingDataset(train_sentences_ids, train_taggings_ids)\n",
        "\n",
        "df_test = PosTaggingDataset(test_sentences_ids, test_taggings_ids)\n",
        "\n",
        "df_valid = PosTaggingDataset(valid_sentences_ids, valid_taggings_ids)\n",
        "\n",
        "print(len(df_train), len(df_test), len(df_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDd47SlBa6Ys",
        "outputId": "a1e31ea9-d70b-45a0-f442-858d9b3bbd7e"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12536 2077 1999\n",
            "time: 1.82 ms (started: 2022-01-21 00:14:12 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "SZpJkNRLwu5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2beec9c-d3e9-467d-a01b-dc40e820454f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.63 ms (started: 2022-01-21 00:14:12 +00:00)\n"
          ]
        }
      ],
      "source": [
        "batch_size=8  ######### TO MODIDY\n",
        "\n",
        "train_loader = DataLoader(df_train, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
        "test_loader = DataLoader(df_test, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
        "valid_loader = DataLoader(df_valid, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "_WdGEKX3lAjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e052fb96-24ad-405b-fee1-b9d3933a3f4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  101,  1996,  4442,  2020,  4082,  1999,  1996,  1043,  3270, 16739,\n",
              "         8717,  2232,  1998,  2632,  1011, 24815,  4733,  1997,  1996,  3007,\n",
              "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 172
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 15.1 ms (started: 2022-01-21 00:14:12 +00:00)\n"
          ]
        }
      ],
      "source": [
        "next(iter(train_loader))[0][7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "053LL-cAtxr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9e97d9e-2e0a-4db7-baa4-3a9d766a6dd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  101,  2632,  1011, 23564,  2386,  1024,  2137,  2749,  2730, 21146,\n",
            "        28209, 14093,  2632,  1011,  2019,  2072,  1010,  1996, 14512,  2012,\n",
            "         1996,  8806,  1999,  1996,  2237,  1997,  1053,  4886,  2213,  1010,\n",
            "         2379,  1996,  9042,  3675,  1012,   102,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0], device='cuda:0')\n",
            "tensor([  101,  1031,  2023,  4288,  1997,  1037,  9768, 29307,  2097,  2022,\n",
            "         4786,  2149,  4390,  2005,  2086,  2000,  2272,  1012,  1033,   102,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0], device='cuda:0')\n",
            "tensor([  101,  1040,  4502,  1024,  8956,  4614,  2623,  2008,  2027,  2018,\n",
            "        23142,  2039,  1017,  9452,  4442,  4082,  1999, 13952,  1012,   102,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0], device='cuda:0')\n",
            "tensor([ 101, 2048, 1997, 2068, 2020, 2108, 2448, 2011, 1016, 4584, 1997, 1996,\n",
            "        3757, 1997, 1996, 4592,  999,  102,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0], device='cuda:0')\n",
            "tensor([  101,  1996, 25175,  1999,  5712,  2003,  5662,  2000,  1996,  2149,\n",
            "         8495,  1010,  2061,  2023,  2052,  2022,  2066,  2383,  1046,  1012,\n",
            "         9586, 17443,  4895,  9148, 13027,  2135, 12666,  2012,  1037,  2152,\n",
            "         2504,  2372,  1997,  1996,  4633,  3549, 10544,  2067,  1999,  1996,\n",
            "         4120,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0], device='cuda:0')\n",
            "tensor([ 101, 1996, 2353, 2001, 2108, 2448, 2011, 1996, 2132, 1997, 2019, 5211,\n",
            "        3813, 1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0], device='cuda:0')\n",
            "tensor([  101,  2017,  4687,  2065,  2002,  2001, 26242,  1996,  3006,  2007,\n",
            "         2010,  8647,  7889,  1012,   102,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0], device='cuda:0')\n",
            "tensor([  101,  1996,  4442,  2020,  4082,  1999,  1996,  1043,  3270, 16739,\n",
            "         8717,  2232,  1998,  2632,  1011, 24815,  4733,  1997,  1996,  3007,\n",
            "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0], device='cuda:0')\n",
            "time: 662 ms (started: 2022-01-21 00:14:12 +00:00)\n"
          ]
        }
      ],
      "source": [
        "o=0\n",
        "\n",
        "for i in list(train_loader)[0]:\n",
        "  o=o+1\n",
        "  if o <2:\n",
        "    for y in i:\n",
        "      print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "IDSN-67jJNAJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d2e671-4f7a-438d-de0f-5640e8ca9c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.4 ms (started: 2022-01-21 00:14:12 +00:00)\n"
          ]
        }
      ],
      "source": [
        "##probing: https://nlp.stanford.edu/~johnhew/interpreting-probes.html\n",
        "#A probe is trained to predict properties we care about from representations of a model whose nature we'd like to know more about.\n",
        "\n",
        "class BERTPoSTagger(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 output_dim, \n",
        "                 dropout):     \n",
        "        super().__init__()       \n",
        "        self.bert = bert      \n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']       \n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)      \n",
        "        self.dropout = nn.Dropout(dropout)       \n",
        "    def forward(self, text): \n",
        "        #text = [sent len, batch size] \n",
        "        text = text.permute(1, 0)     \n",
        "        #text = [batch size, sent len]    \n",
        "        embedded = self.dropout(self.bert(text)[0])     \n",
        "        #embedded = [batch size, seq len, emb dim]             \n",
        "        embedded = embedded.permute(1, 0, 2)                \n",
        "        #embedded = [sent len, batch size, emb dim]   \n",
        "        predictions = self.fc(self.dropout(embedded))    \n",
        "        #predictions = [sent len, batch size, output dim]  \n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "aB4K7worxFpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d7e66f-0068-4280-c2b6-f041a1ffa764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 19])\n",
            "tensor([[[ 0.3852,  0.3417, -0.3667, -0.1071, -0.1895, -0.2648, -0.0144,\n",
            "           0.1781,  0.5827,  0.2060, -0.3581,  0.0288, -0.6476,  0.5256,\n",
            "          -0.7290,  0.4005,  0.5893,  0.3409, -0.3148],\n",
            "         [-0.5497, -0.3651, -0.0570, -0.2743, -0.0017, -0.6323, -0.0526,\n",
            "           0.2110,  0.1679, -0.2160,  0.5215,  0.1663, -0.0519,  0.5612,\n",
            "          -0.5882,  0.4555, -0.1510,  0.3690, -0.5117],\n",
            "         [ 0.4365,  0.1640, -0.4560, -0.1359, -0.2931, -0.3708, -0.1136,\n",
            "           0.3120,  0.4257,  0.2107, -0.4357,  0.0020, -0.6857,  0.5827,\n",
            "          -0.7205,  0.4748,  0.6439,  0.2667, -0.4069]],\n",
            "\n",
            "        [[ 0.1920,  0.1151, -0.3376,  0.0048, -0.0974, -0.0915, -0.1666,\n",
            "           0.1724,  0.6870,  0.2012, -0.3685, -0.1423, -0.7775,  0.4537,\n",
            "          -0.4959,  0.2590,  0.3446,  0.2713, -0.3996],\n",
            "         [-0.0278, -0.2290, -0.1118, -0.0016, -0.0648,  0.3038, -0.0267,\n",
            "           0.1740,  0.1199,  0.1916, -0.1960, -0.1829, -0.4788,  0.0745,\n",
            "          -0.0723,  0.2508, -0.3804, -0.1723, -0.3651],\n",
            "         [ 0.2628, -0.0172, -0.4364, -0.0395, -0.2121, -0.1708, -0.2749,\n",
            "           0.3049,  0.5620,  0.1870, -0.4489, -0.1052, -0.7783,  0.5385,\n",
            "          -0.5187,  0.2884,  0.3981,  0.1915, -0.4959]]])\n",
            "time: 69 ms (started: 2022-01-21 00:14:12 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# check that model works on an arbitrary batch that contains two sentences of length 3\n",
        "dropout=0\n",
        "bert_linear_predict = BERTPoSTagger(bert,output_dim,dropout)\n",
        "\n",
        "#example\n",
        "with torch.no_grad():\n",
        "  y = bert_linear_predict(torch.tensor([[0, 1, 2], [3, 4, 5]]))\n",
        "#the expected shape is (batch size, max sentence length, number of labels)\n",
        "print(y.shape)\n",
        "print(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load Eva's model\n",
        "#childes_bert = torch.load('model.pt')\n",
        "#childes_bert.eval()"
      ],
      "metadata": {
        "id": "t8GF9FUmUDWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4f24c55-ff45-452e-bfbd-7f27c544eb1a"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 613 µs (started: 2022-01-21 00:14:12 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_linear_predict = BERTPoSTagger(bert,output_dim,dropout)\n",
        "\n",
        "bert_linear_predict = bert_linear_predict.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzM-sg2zWHrf",
        "outputId": "84299854-075c-41fb-ce79-46292578b07c"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 177 ms (started: 2022-01-21 00:14:12 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "CQNdV-9QIq6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3f61cc2-ffdc-42dd-fbf6-7a78552eb236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 109,496,851 trainable parameters\n",
            "time: 2.63 ms (started: 2022-01-21 00:14:13 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(bert_linear_predict):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "Dsg5_ek2wYO6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ca9c6fd-1e46-4e33-ee89-539ad7b1670e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 13.5 ms (started: 2022-01-21 00:14:13 +00:00)\n"
          ]
        }
      ],
      "source": [
        "TAG_PAD_IDX = 0\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX) #for multi class classification task\n",
        "\n",
        "def perf(model, loader):\n",
        "  model.eval() # do not apply training-specific steps such as dropout\n",
        "  total_loss = correct = num_loss = num_perf = 0\n",
        "  for x, y in loader:\n",
        "    with torch.no_grad(): # no need to store computation graph for gradients\n",
        "      # perform inference and compute loss\n",
        "      y_scores = model(x)\n",
        "      loss = criterion(y_scores.view(-1, output_dim), y.view(-1)) # requires tensors of shape (num-instances, num-labels) and (num-instances)\n",
        "\n",
        "      # gather loss statistics\n",
        "      total_loss += loss.item() #the loss is usually negative log-likelihood and residual sum of squares for classification and regression respectively. Then naturally, the main objective in a learning model is to reduce (minimize) the loss function's value with respect to the model's parameters by changing the weight vector values through different optimization methods, such as backpropagation in neural networks. Loss value implies how well or poorly a certain model behaves after each iteration of optimization. Ideally, one would expect the reduction of loss after each, or several, iteration(s).\n",
        "\n",
        "      num_loss += 1\n",
        "\n",
        "      # gather accuracy statistics\n",
        "      y_pred = torch.max(y_scores, 2)[1] # compute highest-scoring tag\n",
        "      mask = (y != 0) # ignore <pad> tags\n",
        "      correct += torch.sum((y_pred == y) * mask) # compute number of correct predictions\n",
        "      num_perf += torch.sum(mask).item()\n",
        "  return total_loss / num_loss, correct.item() / num_perf\n",
        "\n",
        "# without training, accuracy should be a bit less than 2% (chance of getting a label correct)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "5rLG3EX_zYKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7bb29b1-9e82-47fc-fb22-706c74ebbafb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT representation (unsupervised) on train dataset (without training) 2.9892431796548014 0.030330837961844964\n",
            "BERT representation (unsupervised) on validation dataset (without training) 2.9903100442886354 0.028443642355525912\n",
            "BERT representation (unsupervised) on test dataset (without training) 2.986672550898332 0.031230358265241988\n",
            "time: 7min 58s (started: 2022-01-21 00:14:13 +00:00)\n"
          ]
        }
      ],
      "source": [
        "print('BERT representation (unsupervised) on train dataset (without training)', *perf(bert_linear_predict, train_loader))\n",
        "print('BERT representation (unsupervised) on validation dataset (without training)', *perf(bert_linear_predict, valid_loader))\n",
        "print('BERT representation (unsupervised) on test dataset (without training)', *perf(bert_linear_predict, test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "DvZ5fZ6Oecdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df983ab-a928-4aa0-bbf5-8f77b02e116b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.47 ms (started: 2022-01-21 00:22:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#need to Fine-tune a BERT model for POS-tagging in the fully supervised setting (use a learning rate of 2e-5, and use a linear warmup learning rate schedule on 10% of the updates as suggested by BERT authors)\n",
        "\n",
        "#Next, we define our optimizer. Usually when fine-tuning you want to use a lower learning rate than normal, this is because we don't want to drastically change the parameters as it may cause our model to forget what it has learned. This phenomenon is called catastrophic forgetting.\n",
        "\n",
        "#We pick 5e-5 (0.00005) as it is one of the three values recommended in the BERT paper. Again, there may be better values for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "W4bRSP5RAuaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "843936fb-8ebd-4b4b-8bb6-6b2ea46d8387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 33.7 ms (started: 2022-01-21 00:22:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#Training is very similar to evaluation as it also performs inference. In addition it uses an optimizer which modifies the parameters of the neural network to minimize the criterion thanks to the gradients accumulated through the forward pass of the model. At each epoch, we perform inference, modify model weights after each batch, and finally use perf to compute loss and accuracy on the validatin data.\n",
        "#Note that training is successful when the training loss gets lower after every epoch. It might fluctuate on validation data because of overtraining or generalization noise.\n",
        "\n",
        "LEARNING_RATE = 5e-5 #### TO MODIFY\n",
        "\n",
        "optimizer = optim.Adam(bert_linear_predict.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "def fit(model, epochs):\n",
        "  best_valid_loss = float('inf')\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = num = correct = num_loss = num_perf = 0\n",
        "    start_time = time.time()\n",
        "    for x, y in train_loader:\n",
        "\n",
        "      optimizer.zero_grad() # start accumulating gradients\n",
        "      y_scores = model(x)\n",
        "      loss = criterion(y_scores.view(-1, output_dim), y.view(-1))\n",
        "      loss.backward() # compute gradients though computation graph\n",
        "      optimizer.step() # modify model parameters\n",
        "      total_loss += loss.item()\n",
        "      num += 1\n",
        "      y_pred = torch.max(y_scores, 2)[1] # compute highest-scoring tag\n",
        "      mask = (y != 0) # ignore <pad> tags\n",
        "      correct += torch.sum((y_pred == y) * mask) # compute number of correct predictions\n",
        "      num_perf += torch.sum(mask).item()\n",
        "\n",
        "    train_loss=total_loss / num \n",
        "    train_acc= correct.item() / num_perf\n",
        "      \n",
        "    end_time = time.time()  \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    valid_loss, valid_acc = perf(model, valid_loader)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut-model.pt')\n",
        "\n",
        "    #print(1 + epoch, total_loss / num, *perf(model, valid_loader))# valid loader instead of train loader SOS SOS\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(1 + epoch, f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(1 + epoch, f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCFAw95XBXqf",
        "outputId": "a45a22b8-fc38-4383-a974-e689239ed3e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 17m 0s\n",
            "1 \tTrain Loss: 0.539 | Train Acc: 82.10%\n",
            "1 \t Val. Loss: 0.456 |  Val. Acc: 85.30%\n",
            "time: 18min (started: 2022-01-21 00:22:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache() \n",
        "#torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "\n",
        "fit(bert_linear_predict, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "6fOIruchLUuF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ecdd551-30d8-44d7-aeab-072b7e6ddabe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 940 µs (started: 2022-01-21 00:40:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#for batch in iter(train_loader):\n",
        "  #print(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdNIZDalYUsJ",
        "outputId": "b0afea4f-d5e1-4aae-84bb-5457d0add047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 2.864 | Test Acc: 11.93%\n",
            "time: 1min 1s (started: 2022-01-21 00:40:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "saved_model = BERTPoSTagger(bert,output_dim,dropout)\n",
        "\n",
        "saved_model = saved_model.to(device)\n",
        "\n",
        "#saved_model.load_state_dict(torch.load('tut-model.pt'))\n",
        "\n",
        "test_loss, test_acc = perf(saved_model, test_loader)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8A7OB9x0ZHj",
        "outputId": "feffc511-e6f6-483d-827b-35622e759ce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['[CLS]', 'al', '-', 'za', '##man', ':', 'american', 'forces', 'killed', 'sha', '##ikh', 'abdullah', 'al', '-', 'an', '##i', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'q', '##ai', '##m', ',', 'near', 'the', 'syrian', 'border', '.', '[SEP]'], ['[CLS]', '[', 'this', 'killing', 'of', 'a', 'respected', 'cleric', 'will', 'be', 'causing', 'us', 'trouble', 'for', 'years', 'to', 'come', '.', ']', '[SEP]']]\n",
            "[101, 1998, 2160, 2175, 102]\n",
            "time: 4.2 ms (started: 2022-01-21 00:41:13 +00:00)\n"
          ]
        }
      ],
      "source": [
        "ex=example[0]\n",
        "to_tokens=[]\n",
        "for t in ex:\n",
        "    to_tokens.append(tokenizer.convert_ids_to_tokens(t))\n",
        "print(to_tokens)\n",
        "\n",
        "to_ids= tokenizer.convert_tokens_to_ids([\"[CLS]\",\"and\", \"house\", \"go\",\"[SEP]\"])\n",
        "print(to_ids)\n",
        "\n",
        "ex=torch.tensor([to_ids]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYroaNjxb2oZ",
        "outputId": "b44d6574-c5c4-4942-dea9-d07e47978533"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[13,  7, 11,  0,  0]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 187
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 19.8 ms (started: 2022-01-21 00:41:13 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def tag_sentence(model, s): #tokenizer,tag_field,text_field, device,   \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      y = saved_model(s)\n",
        "      top_predictions = y.argmax(-1)\n",
        "    return(top_predictions)\n",
        "\n",
        "tag_sentence(saved_model, ex)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test on hand annotations\n",
        "orig_stdout = sys.stdout\n",
        "f = open('out.txt', 'w')\n",
        "sys.stdout = f\n",
        "\n",
        "label_vocab_inv = {y:x for x,y in label_vocab.items()}\n",
        "\n",
        "\n",
        "\n",
        "eng_gold_childes=[]\n",
        "tokenized_eng_gold_childes=[]\n",
        "for sentence in eng_gold_childes_test['utterance_gloss']:\n",
        "  word=sentence.split()\n",
        "  eng_gold_childes.append(word)\n",
        "\n",
        "for s in eng_gold_childes:\n",
        "  tokenized_s = tokenizer.tokenize(' '.join(s))\n",
        "  tokenized_eng_gold_childes.append(tokenized_s)\n",
        "\n",
        "for sen in tokenized_eng_gold_childes:\n",
        "    sen_ = tokenizer.convert_tokens_to_ids(['[CLS]'] + sen + ['[SEP]'])\n",
        "    gold=torch.tensor([sen_]).to(device)\n",
        "    res=tag_sentence(saved_model, gold)\n",
        "    res_c=res.cpu().numpy()[0][2:-2]\n",
        "    print(list(label_vocab_inv[f] for f in res_c))\n",
        "\n",
        "output=[]\n",
        "sys.stdout = orig_stdout\n",
        "f.close()   "
      ],
      "metadata": {
        "id": "vOtifanHy8tX"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate human annotations\n",
        "\n",
        "#df = df.dropna(axis=0, subset=['correct_pos']) \n",
        "mapping = {'mod': 'AUX', 'prep': 'ADP','adj': 'ADJ', 'pro': 'PRON', 'v': 'VERB', 'adv': 'ADV', 'participle': 'VERB', 'n': 'NOUN', 'childes': 'X', 'nan': 'X', 'intj': 'INTJ', 'particle': 'PART', 'mod': 'AUX','mod ': 'AUX', 'spacy': 'X', 'on': 'X', 'participle': 'VERB', 'aux': 'AUX', 'num': 'NUM', 'det':'DET' }\n",
        "#for English, missing PROPN, CCONJ, SCONJ, SYM, PUNCT\n",
        "df = eng_gold_childes_test.replace({'correct_pos': mapping})\n",
        "df = df[['correct_pos', 'position']]"
      ],
      "metadata": {
        "id": "l5AEwO7PXatH"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('out.txt', 'r')\n",
        "correct=0\n",
        "c=0\n",
        "for ind, line in enumerate(f):\n",
        "  print(ind,line)\n",
        "  c+=1\n",
        "  if c<3:\n",
        "    l=list(x.replace(\"[\",\"\").replace(\" \", \"\").replace(\"\\n\",\"\").replace(\"]\",\"\") for x in line.split(\",\"))\n",
        "    position=int(df.iloc[ind]['position'])\n",
        "    print(position, ind)\n",
        "    if l[position][1:-1]== df.iloc[ind]['correct_pos']:\n",
        "      correct+=1\n",
        "  acc=correct/len(df)*100  \n",
        "  print(f'Accuracy on English gold CHILDES: {correct} / {len(df)} , {acc:.2f}%')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "8KRhksKn0Pt5",
        "outputId": "40adb223-6660-40c0-9181-cfc6046ffacf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b4d1575dc41f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'out.txt'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "bert_pos_tagging.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}